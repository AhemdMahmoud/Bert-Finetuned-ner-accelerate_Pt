# -*- coding: utf-8 -*-
"""Ner_System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fh166VMfp_Of8o6hV4gP81JfOIqvTIpw

# references

1- https://spacy.io/api

2- https://youtu.be/2XUhKpH0p4M?si=VlyfU0gPJK03CRVE

3- usecase : https://sigma.ai/named-entity-recognition/#elementor-toc__heading-anchor-2

4:Introduction to Named Entity Recognition :  https://ner.pythonhumanities.com/02_01_spaCy_Entity_Ruler.html   

gliner :https://huggingface.co/urchade/gliner_multi-v2.1

bert: https://huggingface.co/dslim/bert-base-NER


https://colab.research.google.com/drive/1kKhfoEsc-NbhxtpMI5VIBgr22V05OGeg?usp=sharing#scrollTo=RMc-zfehH4bF


https://github.com/aub-mind/arabert

https://chatgpt.com/c/6794ea80-4d74-8008-b318-f25677fd1205
"""

import spacy
from spacy.tokens import Span
from spacy import displacy

nlp=spacy.load("en_core_web_sm")

doc=nlp("im ahmed mahmoud ali from cairo")
for ent in doc.ents:
  print(ent.text,"|",ent.label_,"|",spacy.explain(ent.label_))

nlp.pipe_names

doc[4:5]

displacy.render(doc)

displacy.render(doc,style="ent")

doc.set_ents([Span(doc, 4,5, "PERSON")],default="outside")

# Verify the entity
for ent in doc.ents:
    print(ent.text, "|", ent.label_, "|", spacy.explain(ent.label_))

nlp.pipe_labels['ner']

"""# transformers

"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

from transformers import pipeline

nlp=pipeline("ner",model=model,tokenizer=tokenizer)

doc=nlp("im ahmed mahmoud ali from cairo")

for ent in doc:
  print(ent)

# Load model directly
from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer = AutoTokenizer.from_pretrained("mdarhri00/named-entity-recognition")
model = AutoModelForTokenClassification.from_pretrained("mdarhri00/named-entity-recognition")

nlp=pipeline("ner",model=model,tokenizer=tokenizer)

ner_results = nlp("im ahmed mahmoud ali from cairo")

ner_results

from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

nlp = pipeline("ner", model=model, tokenizer=tokenizer)
example = "Tesla Inc is going to acquire twitter for $45 billion"

ner_results = nlp(example)
print(ner_results)

# from transformers import AutoTokenizer, AutoModelForTokenClassification
# from transformers import pipeline

# tokenizer = AutoTokenizer.from_pretrained("Dizex/InstaFoodRoBERTa-NER")
# model = AutoModelForTokenClassification.from_pretrained("Dizex/InstaFoodRoBERTa-NER")

# pipe = pipeline("ner", model=model, tokenizer=tokenizer)
# example = "Today's meal: Fresh olive poké bowl topped with chia seeds. Very delicious!"

# ner_entity_results = pipe(example, aggregation_strategy="simple")
# print(ner_entity_results)

!python --version

!python --version

#the output
enter=[{
    "token":"ذهب",
    "tags":"O"
  },{
    "token":"محمد",
    "tags":"B-PERS"
  },{
    "token":"إلى",
    "tags":"O"
  },{
    "token":"جامعة",
    "tags":"B-ORG"
  },{
    "token":"بيرزيت",
    "tags":"B-GPE I-ORG"
}]

predict=[]
for ent in enter:
  predict.append([ent["token"],ent["tags"]])
predict

from spacy.tokens import Doc, Span
from spacy import displacy

# Example data
tokens = []
tags = []

for ent in enter:
    tokens.append(ent["token"])
    tags.append(ent["tags"])

# Create a fake Doc object for visualization (SpaCy requires this structure)
nlp = spacy.blank("ar")  # Create a blank Arabic pipeline
doc = Doc(nlp.vocab, words=tokens)

# Add entity spans based on tags
entities = []
for i, (token, tag) in enumerate(zip(tokens, tags)):
    if tag != "O":  # Skip non-entity tokens
        entity = Span(doc, i, i + 1, label=tag)
        entities.append(entity)


# Set the entities on the doc object
doc.ents = entities

# Use Displacy for rendering
html = displacy.render(doc, style="ent", jupyter=False)

# Save to an HTML file
with open("output.html", "w", encoding="utf-8") as file:
    file.write(html)

print("HTML saved as output.html. Open this file in your browser to view the visualization.")

displacy.render(doc, style="ent")

print (tokens)

doc



ner_results

from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt_tab')

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

# Initialize the NER pipeline
nlp = pipeline("ner", model=model, tokenizer=tokenizer)

# Example text
example = "Tesla Inc is going to acquire Twitter for $45 billion"

ent_token=word_tokenize(example)
# Get NER results
ner_results = nlp(example)

# Process the results to group subwords into complete words
entities = []
current_entity = None
current_label = None

for res in ner_results:
    word = res["word"]
    tag = res["entity"]

    if word.startswith("##"):  # Part of a subword
        if current_entity is not None:
            current_entity += word[2:]
        #print
            print ("current_entity:",current_entity)
    else:  # New word
        if current_entity:  # Save the previous entity
            entities.append((current_entity, current_label))
        current_entity = word
        current_label = tag

# Append the last entity
if current_entity:
    entities.append((current_entity, current_label))

ent_token

entity_list=[] # Change the variable name to avoid confusion
# Print the results
tag_2=[]
for entity, tag in entities:
    print(f"{entity}, Tag: {tag}")
    entity_list.append(entity) # Append to the list
    tag_2.append(tag)

entity_list

# Create a fake Doc object for visualization (SpaCy requires this structure)
nlp = spacy.blank("en")  # Create a blank Arabic pipeline
doc = Doc(nlp.vocab, words=ent_token)
print (doc)
# # # Add entity spans based on tags
entities = []
for i, (token, tag) in enumerate(zip(entity_list, tag_2)):  # Use entity_list and tags
    if tag != "O":  # Skip non-entity tokens
        entity = Span(doc, i, i + 1, label=tag)
        print ("entity:",entity)
        entities.append(entity)


# Set the entities on the doc object
doc.ents = entities # Assign entities to doc.ents, not example.ents

# Use Displacy for rendering
displacy.render(doc, style="ent")

doc[0:1]

"""# GLiNER model and"""

!pip install gliner

from gliner import GLiNER

model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

text = """
Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.
"""

labels = ["person", "award", "date", "competitions", "teams"]

entities = model.predict_entities(text, labels)

print(entities)

for entity in entities:
    print(entity["text"], "=>", entity["label"])

text = """
Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.
"""

labels = ["person", "award", "date", "competitions", "teams"]

entities = model.predict_entities(text, labels)

from gliner import GLiNER
import spacy
from spacy.tokens import Doc, Span
from spacy import displacy

# Load the GLiNER model
model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

# Input text
text = """
جامعة بيرزيت وبالتعاون مع مؤسسة ادوارد سعيد تنظم مهرجان للفن الشعبي سيبدأ الساعة الرابعة عصرا، بتاريخ 16/5/2016.
بورصة فلسطين تسجل ارتفاعا بنسبة 0.08% ، في جلسة بلغت قيمة تداولاتها أكثر من نصف مليون دولار .
إنتخاب رئيس هيئة سوق رأس المال وتعديل مادة (4) في القانون الأساسي.
مسيرة قرب باب العامود والذي 700 متر عن المسجد الأقصى.
"""
# nlp=spacy.load("en_core_web_sm")

# Labels for entity extraction
labels = ["person", "organization", "location", "date", "event", "award", "teams", "competitions","percent","Time","currency","Fac","quantity","UNIT","law","DATE"]
#nlp.pipe_labels['ner']

# Predict entities
entities = model.predict_entities(text, labels)
for entity in entities:
    print(entity["text"], "=>", entity["label"])

# Load the spaCy model
nlp = spacy.blank("ar")

# Tokenize the text using spaCy
doc = nlp(text)

# Create a list to hold the Span objects
entities_2 = []

# Iterate over the predicted entities and create Span objects
for entity in entities:
    start = entity["start"]
    end = entity["end"]
    label = entity["label"]
    span = doc.char_span(start, end, label=label)
    if span is not None:
        entities_2.append(span)

# Assign the entities to the doc.ents attribute
doc.ents = entities_2

# Render the entities with displacy
displacy.render(doc, style="ent", jupyter=True)

"""# for english"""

from gliner import GLiNER
import spacy
from spacy.tokens import Doc, Span
from spacy import displacy

# Load the GLiNER model
model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

# Input text
text = """
Tesla Inc is going to acquire Twitter for $45 billion
"""
# nlp=spacy.load("en_core_web_sm")

# Labels for entity extraction
labels = ["person", "organization", "location", "date", "event", "award", "teams", "competitions","percent","Time","currency","Fac","quantity","UNIT","law","DATE","Money"]
#nlp.pipe_labels['ner']

# Predict entities
entities = model.predict_entities(text, labels)
for entity in entities:
    print(entity["text"], "=>", entity["label"])

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

# Tokenize the text using spaCy
doc = nlp(text)

# Create a list to hold the Span objects
entities_2 = []

# Iterate over the predicted entities and create Span objects
for entity in entities:
    start = entity["start"]
    end = entity["end"]
    label = entity["label"]
    span = doc.char_span(start, end, label=label)
    if span is not None:
        entities_2.append(span)

# Assign the entities to the doc.ents attribute
doc.ents = entities_2

# Render the entities with displacy
displacy.render(doc, style="ent", jupyter=True)



from gliner import GLiNER

import spacy
from spacy.tokens import Doc, Span
from spacy import displacy

# Load the GLiNER model
model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")

# Input text
text = """
SpaceX is colaborating with NASA in the mission to bring humans to Mars using their new Starship rocket
"""
nlp=spacy.load("en_core_web_sm")

# Labels for entity extraction
labels = ["person", "organization", "location", "date", "event", "award", "teams", "competitions","percent","Time","currency","Fac","quantity","UNIT","law","DATE"]
#nlp.pipe_labels['ner']

# Predict entities
entities = model.predict_entities(text, labels)
for entity in entities:
    print(entity["text"], "=>", entity["label"])

# Load the spaCy model
# nlp = spacy.blank("ar")

# Tokenize the text using spaCy
doc = nlp(text)

# Create a list to hold the Span objects
entities_2 = []

# Iterate over the predicted entities and create Span objects
for entity in entities:
    start = entity["start"]
    end = entity["end"]
    label = entity["label"]
    span = doc.char_span(start, end, label=label)
    if span is not None:
        entities_2.append(span)

# Assign the entities to the doc.ents attribute
doc.ents = entities_2

# Render the entities with displacy
displacy.render(doc, style="ent", jupyter=True)

# posisible  do this
from gliner_spacy import GLiNERSpacy
nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("GLiNERSpacy", config={"labels": ["person", "organization", "location", "date", "event", "award", "teams", "competitions","percent","Time","currency","Fac","quantity","UNIT","law","DATE"]})

doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)

!pip install gradio

# prompt: from the previouse bulid app in gradio

import gradio as gr

def ner_pipeline(text):
    # Load the GLiNER model
    model = GLiNER.from_pretrained("urchade/gliner_multi-v2.1")
    labels = ["person", "organization", "location", "date", "event", "award", "teams", "competitions","percent","Time","currency","Fac",]
    entities = model.predict_entities(text, labels)

    # Load the spaCy model
    nlp = spacy.blank("ar")  # Use "ar" for Arabic text
    doc = nlp(text)

    entities_2 = []
    for entity in entities:
        start = entity["start"]
        end = entity["end"]
        label = entity["label"]
        span = doc.char_span(start, end, label=label)
        if span is not None:
            entities_2.append(span)
    doc.ents = entities_2
    html = displacy.render(doc, style="ent", jupyter=False)
    return html

iface = gr.Interface(
    fn=ner_pipeline,
    inputs=gr.Textbox(lines=5, placeholder="Enter text here..."),
    outputs=gr.HTML(),
    title="Named Entity Recognition",
    description="Identify named entities in Arabic text using GLiNER and display them with spaCy's displaCy."
)

iface.launch()

entities

nlp=spacy.load("en_core_web_sm")
# Labels for entity extraction
nlp.pipe_labels['ner']

"""# Caml tool"""

pip install camel-tools

!pip install camel-tools[arabert] # install cameltools with arabert model

!sudo apt install tesseract-ocr

import cv2
import pytesseract

# Read the image
img = cv2.imread("/content/images.png")

# Extract text from image
text = pytesseract.image_to_string(img)
print(text)

"""# gliner-spacy: https://github.com/theirstory/gliner-spacy"""

!pip install gliner-spacy

import spacy

nlp = spacy.blank("ar")
nlp.add_pipe("gliner_spacy", config={"labels": ["person", "organization", "location", "date", "event", "award", "teams", "competitions","percent","Time","currency","Fac","quantity","UNIT","law","DATE"]})
text = """
جامعة بيرزيت وبالتعاون مع مؤسسة ادوارد سعيد تنظم مهرجان للفن الشعبي سيبدأ الساعة الرابعة عصرا، بتاريخ 16/5/2016.
بورصة فلسطين تسجل ارتفاعا بنسبة 0.08% ، في جلسة بلغت قيمة تداولاتها أكثر من نصف مليون دولار .
إنتخاب رئيس هيئة سوق رأس المال وتعديل مادة (4) في القانون الأساسي.
مسيرة قرب باب العامود والذي 700 متر عن المسجد الأقصى.
"""
doc = nlp(text)

for ent in doc.ents:
    print(ent.text, ent.label_)

from spacy import displacy

displacy.render(doc, style="ent")

"""# Custom Configs"""

import spacy

custom_spacy_config = { "gliner_model": "urchade/gliner_multi",
                            "chunk_size": 250,
                            "labels": ["people","company"],
                            "style": "ent"}
nlp = spacy.blank("en")
nlp.add_pipe("gliner_spacy", config=custom_spacy_config)

text = "This is a text about Bill Gates and Microsoft."
doc = nlp(text)

for ent in doc.ents:
    print(ent.text, ent.label_, ent._.score)

#Output
# Bill Gates people 0.9967108964920044
# Microsoft company 0.9966742992401123

from spacy import displacy

texts=["This is a text about Bill Gates and Microsoft.","this is another text"]


nlp=spacy.load("en_core_web_sm")

doc=[nlp(text) for text in texts]

print (doc)

entities_2=[]
for ent in doc:
  for entity in ent.ents:
    print(entity.text, entity.label_)
    entities_2.append(entity)







"""# fine tune spacy

"""

# from spacy.util import minibatch, compounding
# from spacy.training import Example
# import spacy
# from spacy.util import minibatch, compounding
# from spacy.training import Example
# import spacy

# nlp = spacy.blank("en") # You need to create a blank spaCy model

# training_data = [
#     ("what is the price of 10 bananas?", {"entities": [(21, 23, "Quantity"), (24, 31, "Product")]}),
#     ("can you tell me the price of 5 apples?", {"entities": [(27, 28, "Quantity"), (29, 35, "Product")]}),
#     ("how much do 12 oranges cost?", {"entities": [(14, 16, "Quantity"), (17, 24, "Product")]}),
#     ("what is the price of a watermelon?", {"entities": [(21, 22, "Quantity"), (23, 33, "Product")]}),
#     ("I need the price for 7 pineapples.", {"entities": [(24, 25, "Quantity"), (26, 36, "Product")]}),
#     ("how much for 15 mangoes?", {"entities": [(13, 15, "Quantity"), (16, 23, "Product")]}),
#     ("check the cost of 20 lemons.", {"entities": [(18, 20, "Quantity"), (21, 27, "Product")]}),
#     ("tell me the price of 6 avocados.", {"entities": [(22, 23, "Quantity"), (24, 32, "Product")]}),
#     ("what is the price of 9 kiwis?", {"entities": [(21, 22, "Quantity"), (23, 28, "Product")]}),
#     ("can you find the price for 3 papayas?", {"entities": [(30, 31, "Quantity"), (32, 39, "Product")]}),
#     ("what does 8 coconuts cost?", {"entities": [(10, 11, "Quantity"), (12, 20, "Product")]}),
#     ("I need the cost for 11 strawberries.", {"entities": [(21, 23, "Quantity"), (24, 35, "Product")]}),
#     ("please tell me the price of 4 peaches.", {"entities": [(26, 27, "Quantity"), (28, 34, "Product")]}),
# ]

import spacy
from spacy.tokens import DocBin

nlp = spacy.blank("en")

training_data = [
    ("what is the price of 10 bananas?", {"entities": [(21, 23, "Quantity"), (24, 31, "Product")]}),
    ("can you tell me the price of 5 apples?", {"entities": [(27, 28, "Quantity"), (29, 35, "Product")]}),
    ("how much do 12 oranges cost?", {"entities": [(14, 16, "Quantity"), (17, 24, "Product")]}),
    ("what is the price of a watermelon?", {"entities": [(21, 22, "Quantity"), (23, 33, "Product")]}),
    ("I need the price for 7 pineapples.", {"entities": [(24, 25, "Quantity"), (26, 36, "Product")]}),
    ("how much for 15 mangoes?", {"entities": [(13, 15, "Quantity"), (16, 23, "Product")]}),
    ("check the cost of 20 lemons.", {"entities": [(18, 20, "Quantity"), (21, 27, "Product")]}),
    ("tell me the price of 6 avocados.", {"entities": [(22, 23, "Quantity"), (24, 32, "Product")]}),
    ("what is the price of 9 kiwis?", {"entities": [(21, 22, "Quantity"), (23, 28, "Product")]}),
    ("can you find the price for 3 papayas?", {"entities": [(30, 31, "Quantity"), (32, 39, "Product")]}),
    ("what does 8 coconuts cost?", {"entities": [(10, 11, "Quantity"), (12, 20, "Product")]}),
    ("I need the cost for 11 strawberries.", {"entities": [(21, 23, "Quantity"), (24, 35, "Product")]}),
    # ("please tell me the price of 4 year old", {"entities": [(26, 27, "Quantity"), (28, 31, "Time")]}),
    ("please tell me the price of 4 year old", {"entities": [(28, 38, "Age")]})
]

import spacy

nlp = spacy.load("en_core_web_sm")

if "ner" not in nlp.pipe_names:
    ner = nlp.add_pipe("ner")
else:
    ner = nlp.get_pipe("ner")

# Add new entity labels
# ner.add_label("NEW_LABEL")

from spacy.training import Example
from spacy.util import minibatch, compounding



# Add labels to the NER component (this step is essential before training)
for _, annotations in training_data:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])  # Add label to the NER component

# Prepare training data
examples = []
for text, annotations in training_data:
    doc = nlp.make_doc(text)
    print (doc)
    examples.append(Example.from_dict(doc, annotations))
    print (examples)

# Training loop
optimizer = nlp.resume_training()
for iteration in range(30):
    losses = {}
    batches = minibatch(examples, size=compounding(4.0, 13.0, 1.001))
    for batch in batches:
        nlp.update(batch, drop=0.2, losses=losses)
    print(f"Iteration {iteration} - Losses: {losses}")

# Save the model
nlp.to_disk("custom_ner_model")

# Load and test
custom_nlp = spacy.load("custom_ner_model")

doc = custom_nlp(" im ahmed mahmoud and im 10 year old ")
for ent in doc.ents:
    print(ent.text, ent.label_)

for ent in doc.ents:
    print(ent.text, ent.label_)

ner.labels

nlp.pipe_names

"""# or use fine tuing via set do but it time consuming"""

import spacy
from spacy import displacy
from spacy.util import minibatch, compounding
from spacy.training import Example

test=" Ahmed is a good boy and your age is 20"

nlp=spacy.load("en_core_web_sm")
doc=nlp(test)
displacy.render(doc, style="ent")

# get index for "is" in test example
for token in doc:
    print(token.text, token.idx)

from spacy.tokens import Span

# get 20 from test
test[37:39]

# # set 20 as quantity
# # Get token index for '20'
# token_index = -1  # Initialize with an invalid index
# for i, token in enumerate(doc):
#     if token.text == "20":
#         token_index = i
#         break

# # Check if '20' was found
# if token_index != -1:
#     # Create span using the token index
#     doc.set_ents([Span(doc, token_index, token_index + 1, "quantity")], default="unmodified")
# else:
#     print("Token '20' not found in the document.")

# set 20 as quantity
doc.set_ents([Span(doc, token_index, token_index + 1, "quantity")], default="unmodified")

for ent in doc.ents:
    if ent.label_ == "PERSON":
        print(ent.text)

for ent in doc.ents:
    print(ent.text, ent.label_)

"""# Pipeline"""

from transformers import pipeline

ner_pipeline = pipeline("ner", model="dslim/bert-base-NER")

ner_pipeline("Ahmed is a good boy and his age is 20")

"""# huggingface-course/bert-finetuned-ner"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForTokenClassification pipeline

tokenizer = AutoTokenizer.from_pretrained("huggingface-course/bert-finetuned-ner")
model = AutoModelForTokenClassification.from_pretrained("huggingface-course/bert-finetuned-ner")

# Load model directly
from transformers import AutoTokenizer, AutoModelForTokenClassification ,pipeline

from os import pipe
pipelines = pipeline("ner", model=model, tokenizer=tokenizer)

ner_rec=pipelines("Ahmed is a good boy and his age is 20")

print(ner_rec)









